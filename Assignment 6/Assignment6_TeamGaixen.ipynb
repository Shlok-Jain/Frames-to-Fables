{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VoKisnaHai1102/Frames-to-Fables/blob/main/Assignment%206/Assignment6_TeamGaixen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11623f8b",
      "metadata": {
        "id": "11623f8b"
      },
      "source": [
        "## Assignment 6\n",
        "This assignment will explore some of the basic and intutive methods that we can use to generate a story out of a sequence of images.\n",
        "\n",
        "The plot is like we will first gnerate detailed captions for each image, then we will use these captions to generate a story using a language model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f14fefb2",
      "metadata": {
        "id": "f14fefb2"
      },
      "source": [
        "#### Image captioning\n",
        "We will use a pre-trained model to generate captions for each image.\n",
        "You have to experiment with different open-sorce image captioning models document and store the results of the captions generated by each model in a file. Update the advantages and disadvantages of each model in the google doc.\n",
        "\n",
        "For sample you can use the following four images:\n",
        "\n",
        "<div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px;\">\n",
        "<img src=\"https://i.ibb.co/RGH8hK6n/Screenshot-2025-06-26-145250.png\" alt=\"Image 1\" width=\"300\">\n",
        "\n",
        "<img src=\"https://i.ibb.co/Mk4mjJhj/Screenshot-2025-06-26-145414.png\" alt=\"Image 2\" width=\"300\">\n",
        "\n",
        "<img src=\"https://i.ibb.co/jvVhChFp/Screenshot-2025-06-26-145429.png\" alt=\"Image 3\" width=\"300\">\n",
        "\n",
        "<img src=\"https://i.ibb.co/8gbYcM0d/Screenshot-2025-06-26-145506.png\" alt=\"Image 4\" width=\"300\">\n",
        "</div>\n",
        "\n",
        "https://i.ibb.co/RGH8hK6n/Screenshot-2025-06-26-145250.png\n",
        "https://i.ibb.co/Mk4mjJhj/Screenshot-2025-06-26-145414.png\n",
        "https://i.ibb.co/jvVhChFp/Screenshot-2025-06-26-145429.png\n",
        "https://i.ibb.co/8gbYcM0d/Screenshot-2025-06-26-145506.png\n",
        "\n",
        "You can also use images of your choice, but keep them same for all the models you are trying, so that you can compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a53d7b",
      "metadata": {
        "id": "a6a53d7b"
      },
      "source": [
        "You have to try atleast 6-7 different models for image captioning, you may choose from the models available on HuggingFace or any other source. Don't stop until you find a model that gives you desired results.\n",
        "\n",
        "You may try variety of models including those based on CLIP, BLIP, Vision-Language Models, Visual Transfrmers, etc. (These are variuos multimodal architectures, you read more about their architectures, uses, etc. online). You can simply google \"image captioning models HuggingFace\" based on any architecture (CLIP, BLIP, VLM, etc.) and you will find many models that you can try.\n",
        "You may also look into their benchmark results to choose most suitable even before trying them out.\n",
        "In my opinion, Vision-Language Models (VLMs) are the most suitable considering the amount of details we want in the captions, but you should try others too.\n",
        "\n",
        "Note: It's ok if you don't completely understand architectures of the models you use, but you should try to understand the basic idea of how they work and if you have further doubts you can reach out to me.\n",
        "Same goes for the code that you will use to generate the captions, you can view how to use the model in the documentation of that particular model, but try to understand the meaning of all the compenents of your code.\n",
        "You may use AI tools to generate the code, but make sure to understand every part of it, else it beats the purpose.\n",
        "Also copy paste the code you used for all the models in this notebook for submission.\n",
        "\n",
        "Also most of the image captioning models on HuggingFace will be large and computationally expensive, so you should not try to run them locally, you can use Goggle Colab, or Kaggle notebooks to run models on their servers (Kaggle provides free GPU for 30 hours per month, so I think that might be enough, even if it's not, that's why I made teams for this assignment, so that you can divide the GPU time across your team members).\n",
        "\n",
        "Another important point is that, some models will be even larger that the computational resources provided by Kaggle or Google colab, so in that case you will have to load the model in quantized format, which will reduce the size of the model and make it easier to run on limited resources.\n",
        "\n",
        "Quantization: It is a technique used to reduce the size of a model by converting its weights from floating-point precision to lower precision, for eg. if originally the weights are in 32-bit floating point format, then you may convert them to 16 or 8 or even 4-bit format. This reduces the memory footprint of the model and makes it easier to run on limited resources. However this may also reduce the performance of the model, so you should try to find a balance between the size and performance of the model.\n",
        "\n",
        "For loading a model in quantized format, there is a llibrary called `bitsandbytes`, for more details on how to use it, you can refer to online resources or ChatGPT, etc.\n",
        "Example code:\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloom-1b7\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f29dba",
      "metadata": {
        "id": "d5f29dba"
      },
      "source": [
        "Here is playlist that I found helpful for understanding huggingface, you may watch the videos that you find helpful:\n",
        "[HuggingFace Playlist](https://www.youtube.com/playlist?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae32ec7",
      "metadata": {
        "id": "fae32ec7"
      },
      "outputs": [],
      "source": [
        "# Code for image captioning\n",
        "# Feel free to use Kaggle for actually running the code, but paste it here after you are done.\n",
        "\n",
        "# model 1\n",
        "# Model name: BLIP-1 Large\n",
        "# Link:\n",
        "# Code:\n",
        "\n",
        "model_id = \"Salesforce/blip-image-captioning-large\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor = BlipProcessor.from_pretrained(model_id)\n",
        "model = BlipForConditionalGeneration.from_pretrained(model_id).to(device)\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Caption for: {filename}\")\n",
        "\n",
        "    image = Image.open(filename).convert(\"RGB\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs)\n",
        "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(caption)\n",
        "\n",
        "# model 2\n",
        "# Model name:\n",
        "# Link:\n",
        "# Code:\n",
        "\n",
        "model_id = \"Salesforce/blip2-flan-t5-xl\"\n",
        "processor = BlipProcessor.from_pretrained(model_id)\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"\\nCaption for: {filename}\")\n",
        "\n",
        "    image = Image.open(filename).convert(\"RGB\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs)\n",
        "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(caption)\n",
        "\n",
        "# model 3\n",
        "# Model name: InstructBLIP (with quantization)\n",
        "# Link:\n",
        "# Code:\n",
        "\n",
        "model_id = \"Salesforce/instructblip-vicuna-7b\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "processor = InstructBlipProcessor.from_pretrained(model_id)\n",
        "model = InstructBlipForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config\n",
        ")\n",
        "\n",
        "for fname in uploaded.keys():\n",
        "\n",
        "    image = Image.open(fname).convert(\"RGB\")\n",
        "    prompt = \"Describe this image in detail.\"\n",
        "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device, torch.float16)\n",
        "    output = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "    caption = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(caption)\n",
        "\n",
        "\n",
        "\n",
        "# model 4\n",
        "# Model name: Git-Base\n",
        "# Link:\n",
        "# Code:\n",
        "\n",
        "print(\" Testing Git-Base (microsoft/git-base)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load model and processor\n",
        "    git_processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n",
        "    git_model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
        "    git_model.to(device)\n",
        "\n",
        "    git_captions = []\n",
        "\n",
        "    for i, image in enumerate(sample_images):\n",
        "        if image is not None:\n",
        "            # Process image\n",
        "            inputs = git_processor(image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            # Generate caption\n",
        "            with torch.no_grad():\n",
        "                generated_ids = git_model.generate(**inputs, max_length=50, num_beams=5)\n",
        "                caption = git_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "                git_captions.append(caption)\n",
        "                print(f\"Image {i+1}: {caption}\")\n",
        "        else:\n",
        "            git_captions.append(\"Failed to load image\")\n",
        "\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    add_result(\n",
        "        model_name=\"Git-Base\",\n",
        "        architecture=\"Generative Image-to-Text Transformer\",\n",
        "        captions=git_captions,\n",
        "        inference_time=f\"{inference_time:.2f}s\",\n",
        "        model_size=\"~139M parameters\",\n",
        "        advantages=\"Very lightweight, fast inference, good for basic captioning, low memory usage\",\n",
        "        disadvantages=\"Limited detail, basic understanding, less sophisticated than newer models\"\n",
        "    )\n",
        "\n",
        "    print(\"âœ“ Git-Base completed successfully!\")\n",
        "\n",
        "    # Clean up memory\n",
        "    del git_model, git_processor\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âœ— Error with Git-Base: {e}\")\n",
        "    add_result(\"Git-Base\", \"Generative Image-to-Text\", [\"Error\"]*4, \"N/A\", \"N/A\", \"N/A\", f\"Failed to load: {e}\")\n",
        "\n",
        "# model 5\n",
        "# Model name:blip2-opt-2.7b\n",
        "# Link:\n",
        "# Code:\n",
        "print(\"Testing BLIP-2 OPT-2.7B - Salesforce/blip2-opt-2.7b\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load model and processor\n",
        "    blip2_opt_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "    blip2_opt_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "    blip2_opt_model.to(device)\n",
        "\n",
        "    blip2_opt_captions = []\n",
        "\n",
        "    for i, image in enumerate(sample_images):\n",
        "        if image is not None:\n",
        "            # Process image\n",
        "            inputs = blip2_opt_processor(image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            # Generate caption\n",
        "            with torch.no_grad():\n",
        "                generated_ids = blip2_opt_model.generate(**inputs, max_length=100, num_beams=5)\n",
        "                caption = blip2_opt_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "                blip2_opt_captions.append(caption)\n",
        "                print(f\"Image {i+1}: {caption}\")\n",
        "        else:\n",
        "            blip2_opt_captions.append(\"Failed to load image\")\n",
        "\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    add_result(\n",
        "        model_name=\"BLIP-2-OPT-2.7B\",\n",
        "        architecture=\"BLIP-2 with OPT Language Model\",\n",
        "        captions=blip2_opt_captions,\n",
        "        inference_time=f\"{inference_time:.2f}s\",\n",
        "        model_size=\"~3.5B parameters\",\n",
        "        advantages=\"Good balance of size and performance, faster than larger BLIP-2 variants\",\n",
        "        disadvantages=\"Less detailed than larger models, may miss fine details\"\n",
        "    )\n",
        "\n",
        "    # Clean up memory\n",
        "    del blip2_opt_model, blip2_opt_processor\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error with BLIP-2-OPT: {e}\")\n",
        "    add_result(\"BLIP-2-OPT-2.7B\", \"BLIP-2 with OPT\", [\"Error\"]*4, \"N/A\", \"N/A\", \"N/A\", f\"Failed to load: {e}\")\n",
        "\n",
        "# model 6\n",
        "# Model name:blip2-flan-t5-xl\n",
        "# Link:\n",
        "# Code:\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
        "\n",
        "img_url = \"https://i.ibb.co/RGH8hK6n/Screenshot-2025-06-26-145250.png\"\n",
        "image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
        "\n",
        "# Step 3: Prepare Input and Generate Caption\n",
        "inputs = processor(image, return_tensors=\"pt\")\n",
        "out = model.generate(**inputs, max_new_tokens=50)\n",
        "caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Caption:\", caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cb9911c",
      "metadata": {
        "id": "7cb9911c"
      },
      "source": [
        "Update the google doc, and also store the captions generated by each model in a file, you can use any format you like, but I would suggest using CSV format, as it is easy to handle using pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23653b8a",
      "metadata": {
        "id": "23653b8a"
      },
      "source": [
        "#### Story Generation\n",
        "Once you have generated the captions for each image, you can use a language model to generate a story using these captions.\n",
        "\n",
        "Again you have to experiment with different open-source Large Language Models(LLMs) and document the results in the google doc.\n",
        "\n",
        "LLMs are very powerful models for language generation tasks, these are based on transformer architecture that we studied in past sessions. And they have been trained on a large corpus of text data, so they are capable of handling a wide range of language tasks, including text generation, text classification, text summarization, etc.\n",
        "\n",
        "One of the proprietary LLMs that you use in your daily life is ChatGPT, you can also use that for this, but the problem is that it's API is not free, so for automating the process you will have to use open-source LLMs again from Hugging Face.\n",
        "\n",
        "As done in captioning, you have to try out atleast 6-7 different LLMs for story generation, some examples include:\n",
        "- Meta's Llama (including different versions, as well as various finetuned versions of it)\n",
        "- Mistral\n",
        "- Gemini API (This is free upto a certain limit, so can try it out)\n",
        "\n",
        "You can find many more here: [LLM leaderboard](https://huggingface.co/collections/open-llm-leaderboard/open-llm-leaderboard-best-models-652d6c7965a4619fb5c27a03) or [here](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b8c5f53",
      "metadata": {
        "id": "1b8c5f53"
      },
      "source": [
        "You can find details on how to use these models on their respective Hugging Face pages, or external documentations if any.\n",
        "\n",
        "The things that I told about quantization in image captioning also applies to LLMs, so you can use `bitsandbytes` library to load the models in quantized format if they are too large to run on your resources.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "402aa5e2",
      "metadata": {
        "id": "402aa5e2"
      },
      "source": [
        "One of the other important skill is Prompt Engineering, which is the art of crafting effective prompts to get the desired output from a language model.\n",
        "\n",
        "You won't get your desired output in the first go, so you will have to experiment with different prompts and see which one works best for your task, and also the model you are using.\n",
        "You can use the following prompt as a starting point:\n",
        "```python\n",
        "prompt = f\"\"\"\n",
        "You are a creative writer, and you have to write a story based on captions of a sequence of images.\n",
        "{captions}\n",
        "Write a detailed story that connects these captions in a meaningful way.\n",
        "\"\"\"\n",
        "```\n",
        "You can also instruct to include specific elements in the story, like characters, setting, etc. or to follow a particular style or tone.\n",
        "Note that this is a very basic prompt, tweak it so as to get the best output from the model you are using, experimentation is the key here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6b44e7",
      "metadata": {
        "id": "9b6b44e7"
      },
      "source": [
        "Very IMP: Make sure to store the results of captions, as well as story that you generated after loading these models in a file, so that you won't have to run the models again and again, you will eventually realise that loading  these models i svery tedious and time consuming, also you will have to face a lot of errors while doing so, so you will also learn how to debug these errors, which is a very important skill in itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6be26c",
      "metadata": {
        "id": "fe6be26c"
      },
      "outputs": [],
      "source": [
        "# Code for Story Generation\n",
        "# Feel free to use Kaggle or Colab for actually running the code, but paste it here after you are done.\n",
        "\n",
        "# model 1\n",
        "# Model name: Qwen2.5-3B\n",
        "# Link:\n",
        "# Code:\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"Qwen/Qwen2.5-3B\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "captions = \"\"\"\n",
        "You are a creative storyteller. Based on the following image captions, write a heartfelt and imaginative story that connects all the scenes in a meaningful way. Maintain the flow and emotions across scenes.\n",
        "\n",
        "1. The image depicts two young boys, one of whom is holding a toy plane, while the other is trying to take it away from him. The scene is set in a living room, with a TV and a chair visible in the background. The boys appear to be engaged in a playful argument, with the one holding the plane trying to keep it from the other boy. The emotions involved in this scene are likely a mix of playfulness, competition, and possibly some conflict.\n",
        "\n",
        "2. The image depicts a cartoon scene of a mother and two children in a living room. The mother is holding a toy airplane, and the children are looking at her with a mixture of confusion and fear. The mother is trying to calm them down, possibly by explaining the situation or reassuring them. The overall atmosphere is one of concern and uncertainty, as the children seem to be unsure of what is happening.\n",
        "\n",
        "3. The image depicts two young boys sitting at a dining table, each holding a paper airplane. They seem to be enjoying themselves, possibly playing with the paper airplanes or engaging in a creative activity together. The scene is characterized by a sense of fun and playfulness, as the boys are having a good time with their paper airplane creations.\n",
        "\n",
        "4. The image depicts a cartoon scene of a mother and two children playing with paper airplanes in a living room. The mother is helping the children with their paper airplane-making skills, and they are enjoying the activity together. The scene is filled with joy and laughter, as the family engages in a fun and creative activity. The mother is likely teaching the children how to make paper airplanes, which can be a fun and educational activity for kids.\n",
        "\n",
        "Now write a story using these captions as plot points. Make it flow naturally like a childrenâ€™s story. Use friendly and descriptive language. Bring out the emotions (joy, conflict, bonding) across the scenes.\n",
        "\"\"\"\n",
        "\n",
        "output = generator(\n",
        "    captions,\n",
        "    max_new_tokens=800,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "print(output[0]['generated_text'])\n",
        "\n",
        "# model 2\n",
        "# Model name: Google flan-t5-large\n",
        "# Link:\n",
        "# Code:\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-large\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "captions = \"\"\"\n",
        "You are a creative storyteller. Based on the following image captions, write a heartfelt and imaginative story that connects all the scenes in a meaningful way. Maintain the flow and emotions across scenes.\n",
        "\n",
        "1. The image depicts two young boys, one of whom is holding a toy plane, while the other is trying to take it away from him. The scene is set in a living room, with a TV and a chair visible in the background. The boys appear to be engaged in a playful argument, with the one holding the plane trying to keep it from the other boy. The emotions involved in this scene are likely a mix of playfulness, competition, and possibly some conflict.\n",
        "\n",
        "2. The image depicts a cartoon scene of a mother and two children in a living room. The mother is holding a toy airplane, and the children are looking at her with a mixture of confusion and fear. The mother is trying to calm them down, possibly by explaining the situation or reassuring them. The overall atmosphere is one of concern and uncertainty, as the children seem to be unsure of what is happening.\n",
        "\n",
        "3. The image depicts two young boys sitting at a dining table, each holding a paper airplane. They seem to be enjoying themselves, possibly playing with the paper airplanes or engaging in a creative activity together. The scene is characterized by a sense of fun and playfulness, as the boys are having a good time with their paper airplane creations.\n",
        "\n",
        "4. The image depicts a cartoon scene of a mother and two children playing with paper airplanes in a living room. The mother is helping the children with their paper airplane-making skills, and they are enjoying the activity together. The scene is filled with joy and laughter, as the family engages in a fun and creative activity. The mother is likely teaching the children how to make paper airplanes, which can be a fun and educational activity for kids.\n",
        "\n",
        "Now write a story using these captions as plot points. Make it flow naturally like a childrenâ€™s story. Use friendly and descriptive language. Bring out the emotions (joy, conflict, bonding) across the scenes.\n",
        "\"\"\"\n",
        "\n",
        "output = generator(\n",
        "    captions,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(output[0]['generated_text'])\n",
        "\n",
        "# model 3\n",
        "# Model name: Qwen2.5-7B\n",
        "# Link:\n",
        "# Code:\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"Qwen/Qwen2.5-7B\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "captions = \"\"\"\n",
        "You are a creative storyteller. Based on the following image captions, write a heartfelt and imaginative story that connects all the scenes in a meaningful way. Maintain the flow and emotions across scenes.\n",
        "\n",
        "1. The image depicts two young boys, one of whom is holding a toy plane, while the other is trying to take it away from him. The scene is set in a living room, with a TV and a chair visible in the background. The boys appear to be engaged in a playful argument, with the one holding the plane trying to keep it from the other boy. The emotions involved in this scene are likely a mix of playfulness, competition, and possibly some conflict.\n",
        "\n",
        "2. The image depicts a cartoon scene of a mother and two children in a living room. The mother is holding a toy airplane, and the children are looking at her with a mixture of confusion and fear. The mother is trying to calm them down, possibly by explaining the situation or reassuring them. The overall atmosphere is one of concern and uncertainty, as the children seem to be unsure of what is happening.\n",
        "\n",
        "3. The image depicts two young boys sitting at a dining table, each holding a paper airplane. They seem to be enjoying themselves, possibly playing with the paper airplanes or engaging in a creative activity together. The scene is characterized by a sense of fun and playfulness, as the boys are having a good time with their paper airplane creations.\n",
        "\n",
        "4. The image depicts a cartoon scene of a mother and two children playing with paper airplanes in a living room. The mother is helping the children with their paper airplane-making skills, and they are enjoying the activity together. The scene is filled with joy and laughter, as the family engages in a fun and creative activity. The mother is likely teaching the children how to make paper airplanes, which can be a fun and educational activity for kids.\n",
        "\n",
        "Now write a story using these captions as plot points. Make it flow naturally like a childrenâ€™s story. Use friendly and descriptive language. Bring out the emotions (joy, conflict, bonding) across the scenes.\n",
        "\"\"\"\n",
        "\n",
        "output = generator(\n",
        "    captions,\n",
        "    max_new_tokens=800,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "print(output[0]['generated_text'])\n",
        "\n",
        "# model 4\n",
        "# Model name:TinyLlama-1.1B-Chat-v1.0\n",
        "# Link:\n",
        "# Code:\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "captions = [\n",
        "    \"a cartoon of two children playing with toys in the living room.\",\n",
        "    \"a cartoon showing family with two children\",\n",
        "    \"two boys are making paper aeroplanes at table.\",\n",
        "    \"a cartoon of a family with two children playing with paper aeroplanes.\"\n",
        "]\n",
        "\n",
        "for caption in captions:\n",
        "    prompt = f\"Generate a short, creative, and engaging story based on this scene description:\\n{caption}\\nStory:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    output = model.generate(**inputs, max_new_tokens=200)\n",
        "    story = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"\\nCaption: {caption}\")\n",
        "    print(f\"Generated Story:\\n{story}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "422ac20e",
      "metadata": {
        "id": "422ac20e"
      },
      "source": [
        "The End! ðŸ«¡\n",
        "Hope you enjoyed this assignment, and learned something new."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}